{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKUXcOg67oI_"
      },
      "source": [
        "## Students Information\n",
        "\n",
        "Please enter the names and IDs of the two students below:\n",
        "\n",
        "1. **Name**: [Enter Student 1 Name Here]  \n",
        "   **ID**: `9XXXXXX`\n",
        "\n",
        "2. **Name**: [Enter Student 2 Name Here]  \n",
        "   **ID**: `9XXXXXX`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektm8QNe7oJA"
      },
      "source": [
        "## Students Instructions\n",
        "\n",
        "This is your last graded lab assignment, as you put the work you have studied in the lectures in action, please take this opportunity to enhance your understanding of the concepts and hone your skills. As you work on your assignment, please keep the following instructions in mind:\n",
        "\n",
        "- Clearly state your personal information where indicated.\n",
        "- Be ready with your work before the time of the next discussion slot in the schedule.\n",
        "- Plagiarism will be met with penalties, refrain from copying any answers to make the most out of the assignment. If any signs of plagiarism are detected, actions will be taken.\n",
        "- It is acceptable to share the workload of the assignment bearing the discussion in mind.\n",
        "- Feel free to [reach out](mailto:cmpsy27@gmail.com) if there were any ambiguities or post on the classroom."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4hR0ZRg7oJB"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "To ensure a smooth evaluation process, please follow these steps for submitting your work:\n",
        "\n",
        "1. **Prepare Your Submission:** Alongside your main notebook, include any additional files that are necessary for running the notebook successfully. This might include data files, images, or supplementary scripts.\n",
        "\n",
        "2. **Rename Your Files:** Before submission, please rename your notebook to reflect the IDs of the two students working on this assignment. The format should be `ID1_ID2`, where `ID1` and `ID2` are the student IDs. For example, if the student IDs are `9123456` and `9876543`, then your notebook should be named `9123456_9876543.ipynb`.\n",
        "\n",
        "3. **Check for Completeness:** Ensure that all required tasks are completed and that the notebook runs from start to finish without errors. This step is crucial for a smooth evaluation.\n",
        "\n",
        "4. **Submit Your Work:** Once everything is in order, submit your notebook and any additional files via the designated submission link on Google Classroom **(code: 2yj6e24)**. Make sure you meet the submission deadline to avoid any late penalties.\n",
        "5. Please, note that the same student should submit the assignments for the pair throughout the semester.\n",
        "\n",
        "By following these instructions carefully, you help us in evaluating your work efficiently and fairly **and any failure to adhere to these guidelines can affect your grades**. If you encounter any difficulties or have questions about the submission process, please reach out as soon as possible.\n",
        "\n",
        "We look forward to seeing your completed assignments and wish you the best of luck!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEs8d9aO7oJB"
      },
      "source": [
        "## Installation Instructions\n",
        "\n",
        "In this lab assignment, we require additional Python libraries for machine learning (ML) and deep learning (DL) algorithms and frameworks. To fulfill these requirements, we need to install Pytorch.\n",
        "1. Install Pytorch \\\n",
        "PyTorch is a versatile and powerful machine learning library for Python, known for its flexibility and ease of use in research and production. It supports various deep learning operations and models, including convolutional and recurrent neural networks. For Windows users, the installation also requires ensuring that CUDA, provided by NVIDIA, is compatible to enable GPU acceleration. This enhances performance significantly, particularly in training large neural networks.\\\n",
        "For windows installation with GPU support you can [check out this link](https://pytorch.org/get-started/locally/) which is the source for the command below and please know that support for GPU is done for windows so you can also check out [previous versions](https://pytorch.org/get-started/previous-versions/), you could use CPU on windows smoothly, use linux or resort to [WSL](https://www.youtube.com/watch?v=R4m8YEixidI).\n",
        "\n",
        "```bash\n",
        "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "```\n",
        "```bash\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUEZhvFj7oJC"
      },
      "source": [
        "> **Note:** You are allowed to install any other necessary libraries you deem useful for solving the lab. Please ensure that any additional libraries are compatible with the assignment requirements and are properly documented in your submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ScA3A57oJC"
      },
      "source": [
        "## U-Net\n",
        "\n",
        "U-Net is a convolutional neural network designed specifically for biomedical image segmentation. First introduced in a 2015 paper by Olaf Ronneberger, Philipp Fischer, and Thomas Brox, its architecture is notably shaped like the letter \"U\", which inspired its name. This structure comprises a contracting path to capture context and a symmetrically expanding path that aids in precise localization, making it particularly adept at leveraging small datasets to achieve highly accurate segmentations. U-Net's ability to accurately segment various tissues and medical conditions has made it a staple in medical imaging and has spurred adaptations for broader image analysis applications. Its success in the medical domain demonstrates its potential for high-impact applications in other fields requiring detailed image segmentation.\n",
        "\n",
        "U-Net's robust architecture has found significant applications in the field of satellite imaging as well. The ability of U-Net to effectively handle multi-scale and high-resolution images makes it particularly suitable for satellite image analysis. In satellite imaging, U-Net is commonly used for tasks such as land cover classification, road detection, and building segmentation. Its structure allows for precise segmentation of complex objects from high-resolution satellite imagery, even with relatively limited labeled datasets. This capability is crucial in environmental monitoring, urban planning, and disaster management, where accurate and detailed image analysis is required. The adaptability and efficiency of U-Net in handling spatial hierarchies and various textures present in satellite images highlight its versatility and effectiveness beyond its initial medical context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnzwBa6Y7oJC"
      },
      "source": [
        "### U-Net Architecture\n",
        "\n",
        "![U-Net Architecture](unet.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBU4-eag7oJC"
      },
      "source": [
        "### Detailed Overview of Functions in `utilities.py`\n",
        "\n",
        "The `utilities.py` file contains multiple functions to support image processing, data manipulation, and neural network training:\n",
        "\n",
        "- **`generate_random_data(height, width, count)`**:\n",
        "  - Generates random images and masks.\n",
        "\n",
        "- **`generate_img_and_mask(height, width)`**:\n",
        "  - Generates a single image and corresponding mask.\n",
        "\n",
        "- **`add_square(img, mask)`**:\n",
        "  - Adds a square to an image and updates the mask.\n",
        "\n",
        "- **`add_filled_square(img, mask)`**:\n",
        "  - Adds a filled square to an image and updates the mask.\n",
        "\n",
        "- **`logical_and(mask1, mask2)`**:\n",
        "  - Performs logical 'AND' operation between two masks.\n",
        "\n",
        "- **`add_mesh_square(img, mask)`**:\n",
        "  - Adds a mesh-patterned square to an image and updates the mask.\n",
        "\n",
        "- **`add_triangle(img, mask)`**:\n",
        "  - Adds a triangle to an image and updates the mask.\n",
        "\n",
        "- **`add_circle(img, mask)`**:\n",
        "  - Adds a circle to an image and updates the mask.\n",
        "\n",
        "- **`add_plus(img, mask)`**:\n",
        "  - Adds a plus sign to an image and updates the mask.\n",
        "\n",
        "- **`get_random_location(shape, zoom)`**:\n",
        "  - Gets a random location within an image shape.\n",
        "\n",
        "- **`plot_img_array(img_array, n_col)`**:\n",
        "  - Plots an array of images.\n",
        "\n",
        "- **`plot_side_by_side(img_arrays)`**:\n",
        "  - Plots images side by side for comparison.\n",
        "\n",
        "- **`plot_errors(train_losses, val_losses)`**:\n",
        "  - Plots training and validation loss errors.\n",
        "\n",
        "- **`masks_to_colorimg(masks)`**:\n",
        "  - Converts masks to colored images.\n",
        "\n",
        "- **`generate_images_and_masks_then_plot()`**:\n",
        "  - Generates images and masks and plots them.\n",
        "\n",
        "- **`reverse_transform(transform)`**:\n",
        "  - Reverses a transformation applied to an image.\n",
        "\n",
        "- **`get_data_loaders(train_dir, valid_dir, batch_size)`**:\n",
        "  - Gets data loaders for training and validation datasets.\n",
        "\n",
        "- **`dice_loss(inputs, targets, smooth)`**:\n",
        "  - Calculates Dice loss for model evaluation.\n",
        "\n",
        "- **`calc_loss(pred, target, metrics)`**:\n",
        "  - Calculates loss and updates metrics.\n",
        "\n",
        "- **`print_metrics(metrics, epoch_samples, phase)`**:\n",
        "  - Prints metrics for training or validation phases.\n",
        "\n",
        "- **`train_model(model, dataloaders, criterion, optimizer, num_epochs)`**:\n",
        "  - Trains a model given data loaders and training parameters.\n",
        "\n",
        "- **`run(model)`**:\n",
        "  - Runs a full model training and evaluation session.\n",
        "\n",
        "- **`__init__`, `__len__`, `__getitem__`**:\n",
        "  - Standard class methods used in Python classes, typically for data handling or model initialization.\n",
        "\n",
        "These functions collectively facilitate the full lifecycle of processing, training, and evaluating neural network models, particularly focusing on image-related tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1A58lnv7oJD"
      },
      "source": [
        "The primary objective of this assignment is to develop a neural network model capable of performing accurate image segmentation. Image segmentation involves dividing an image into segments that represent different objects or regions, which is crucial in various applications such as medical imaging, autonomous driving, and satellite image analysis.\n",
        "\n",
        "#### Image and Mask Generation\n",
        "In this assignment, synthetic images along with corresponding segmentation masks are generated to train and evaluate the segmentation model. Each image typically includes multiple geometric shapes placed randomly. These shapes can include simple forms such as circles, squares, and triangles, or more complex designs.\n",
        "\n",
        "#### Masks and Their Role in Segmentation\n",
        "- **Mask Generation**: Alongside each synthetic image, a mask is generated where each shape in the image has a corresponding segment in the mask. Each segment in the mask is represented by a unique color or grayscale intensity, where each intensity level corresponds to a different class (shape).\n",
        "- **Function of the Mask**: The mask serves as a \"ground truth\" for training the segmentation model. The model learns to predict these masks from the input images. Essentially, the task of the model is to map the input image to its corresponding output mask, segmenting the shapes it has learned during training.\n",
        "- **Learning Process**: During training, the model adjusts its parameters to minimize the difference between its predicted masks and the ground truth masks. This process involves optimizing a loss function, typically using backpropagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC-YV-5q7oJD"
      },
      "source": [
        "## Requirement - U-Net Application\n",
        "- **Implement or Use U-Net**:\n",
        "  - **Option 1: Implementation** - Students are encouraged to implement the U-Net architecture from scratch, including defining all the layers and connections based on the standard specifications found in the literature.\n",
        "  - **Option 2: Use Pre-existing Implementation** - Students may also opt to use an existing implementation of U-Net. This can include adapting an open-source model available in frameworks like PyTorch.\n",
        "\n",
        "- **Integration with `utilities.py`**:\n",
        "  - The U-Net model, whether self-implemented or pre-existing, must be integrated with the `utilities.py` script provided. This integration is essential for processing data, training the model, and evaluating its performance efficiently.\n",
        "  - The model should effectively process input images and produce accurate segmentation masks as output, corresponding to the different segments of the input images.\n",
        "  - **You can make any necessary modifications including writing your own training loop for example**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4wcPi65i7oJD"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V_85LD5A7oJE"
      },
      "outputs": [],
      "source": [
        "# import libraries here\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.nn.functional import relu,softmax\n",
        "import sys\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mE4RPyU7oJE"
      },
      "source": [
        "Convolution Size Rule\n",
        "```\n",
        "Wout = (W-F+2P)/S+1\n",
        "```\n",
        "\n",
        "UpConvolution Size Rule\n",
        "```\n",
        "W = (Wout-1)*S+F-2P\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KjM3sODJ7oJF"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "        # TODO: Define needed layers, use n_class variable in the last layer.'\n",
        "\n",
        "        # Encoder (Contracting Path): down sampling the input image size while depth increases\n",
        "        # Learning the WHAT (features) information in the image , but losing the WHERE (spatial) information\n",
        "        # Each Block :\n",
        "        #   1. Two 3*3 Convolutional Layer zero-padded with stride=1 Each Followed by a RELU Activation\n",
        "        #   2. Max Pooling Layer 2*2 with stride=2 (Dimension halved)(Same Depth) [⬇ Down Sampling] [Except the last block :D]\n",
        "\n",
        "        # input: 572x572x3\n",
        "        self.e_1_conv_1= nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=0, bias=False)   # 570x570x64\n",
        "        self.e_1_conv_2= nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0, bias=False)  # 568x568x64\n",
        "        self.e_1_pool= nn.MaxPool2d(kernel_size=2, stride=2)  # 284x284x64\n",
        "\n",
        "        # input: 284x284x64\n",
        "        self.e_2_conv_1= nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0, bias=False)   # 282x282x128\n",
        "        self.e_2_conv_2= nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0, bias=False)   # 280x280x128\n",
        "        self.e_2_pool= nn.MaxPool2d(kernel_size=2, stride=2)  # 140x140x128\n",
        "\n",
        "\n",
        "        # input: 140x140x128\n",
        "        self.e_3_conv_1= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0, bias=False)   # 138x138x256\n",
        "        self.e_3_conv_2= nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=0, bias=False)   # 136x136x256\n",
        "        self.e_3_pool= nn.MaxPool2d(kernel_size=2, stride=2,padding=1)  # 68x68x256\n",
        "\n",
        "\n",
        "        # input: 68x68x256\n",
        "        self.e_4_conv_1= nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=0, bias=False)   # 66x66x512\n",
        "        self.e_4_conv_2= nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=0, bias=False)   # 64x64x512\n",
        "        self.e_4_pool= nn.MaxPool2d(kernel_size=2, stride=2,padding=1)  # 32x32x512\n",
        "\n",
        "\n",
        "        # input: 32x32x512\n",
        "        self.e_5_conv_1= nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=0, bias=False)   # 30x30x1024\n",
        "        self.e_5_conv_2= nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=0, bias=False)   # 28x28x1024\n",
        "        # No Max Pooling Layer\n",
        "\n",
        "\n",
        "        # Decoder (Expansive Path): Up sampling feature maps to the original input image size, Size increases while Depth decreases\n",
        "        # Learning the Where (spatial) information in the image.\n",
        "        # Each Block :\n",
        "        #   1. Up Convolution Layer 2*2 width stride=2 and zero pad (Dimension doubled)(Depth halved)[⬆ Up Sampling]\n",
        "        #   2. Two 3*3 Convolutional Layer zero-padded with stride=1 Each Followed by a RELU Activation\n",
        "        #   3. + Conv 1x1 @Last Block to reduce the depth to the number of classes\n",
        "\n",
        "        # input: 28x28x1024\n",
        "        self.d_1_up= nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2, padding=0,output_padding=0, bias=False) # 56x56x512\n",
        "        #  56x56x512[From Encoder] + 56x56x512 = 56x56x1024\n",
        "        self.d_1_conv_1= nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=0, bias=False) # 54x54x512\n",
        "        self.d_1_conv_2= nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=0, bias=False) # 52x52x512\n",
        "\n",
        "        # input:  52x52x512\n",
        "        self.d_2_up= nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2, padding=0,output_padding=0, bias=False) # 104x104x256\n",
        "        #  104x104x256[From Encoder] + 104x104x256 = 104x104x512\n",
        "        self.d_2_conv_1= nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=0, bias=False) # 102x102x256\n",
        "        self.d_2_conv_2= nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=0, bias=False) # 100x100x256\n",
        "\n",
        "        # input:  100x100x256\n",
        "        self.d_3_up= nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2, padding=0,output_padding=0, bias=True) # 200x200x128\n",
        "        #  200x200x128[From Encoder] + 200x200x128  = 200x200x256\n",
        "        self.d_3_conv_1= nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=0, bias=True) # 198x198x128\n",
        "        self.d_3_conv_2= nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=0, bias=True) # 196x196x128\n",
        "\n",
        "        # input:  196x196x128\n",
        "        self.d_4_up= nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2, padding=0,output_padding=0, bias=False) # 392x392x64\n",
        "        #  392x392x64[From Encoder] + 392x392x64  = 392x392x128\n",
        "        self.d_4_conv_1= nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=0, bias=False) # 390x390x64\n",
        "        self.d_4_conv_2= nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0, bias=False) # 388x388x64\n",
        "\n",
        "        # input:  388x388x64\n",
        "        self.d_4_conv_3= nn.Conv2d(in_channels=64, out_channels=n_class, kernel_size=1, stride=1, padding=0, bias=False) # 388x388xn_class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Define forward function\n",
        "        # Encoder (Contracting Path)\n",
        "        x_e_1_conv_1 = relu(self.e_1_conv_1(x))\n",
        "        x_e_1_conv_2 = relu(self.e_1_conv_2(x_e_1_conv_1))\n",
        "        x_e_1_pool = self.e_1_pool(x_e_1_conv_2)\n",
        "\n",
        "        x_e_2_conv_1 = relu(self.e_2_conv_1(x_e_1_pool))\n",
        "        x_e_2_conv_2 = relu(self.e_2_conv_2(x_e_2_conv_1))\n",
        "        x_e_2_pool = self.e_2_pool(x_e_2_conv_2)\n",
        "\n",
        "        x_e_3_conv_1 = relu(self.e_3_conv_1(x_e_2_pool))\n",
        "        x_e_3_conv_2 = relu(self.e_3_conv_2(x_e_3_conv_1))\n",
        "        x_e_3_pool = self.e_3_pool(x_e_3_conv_2)\n",
        "\n",
        "        x_e_4_conv_1 = relu(self.e_4_conv_1(x_e_3_pool))\n",
        "        x_e_4_conv_2 = relu(self.e_4_conv_2(x_e_4_conv_1))\n",
        "        x_e_4_pool = self.e_4_pool(x_e_4_conv_2)\n",
        "\n",
        "        x_e_5_conv_1 = relu(self.e_5_conv_1(x_e_4_pool))\n",
        "        x_e_5_conv_2 = relu(self.e_5_conv_2(x_e_5_conv_1))\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        x_d_1_up= self.d_1_up(x_e_5_conv_2)\n",
        "        crop_shape=x_d_1_up.shape\n",
        "        x_e_4_conv_2_cropped = x_e_4_conv_2[:, :, :crop_shape[2], :crop_shape[3]]\n",
        "        x_d_1_up_concat = torch.cat([x_e_4_conv_2_cropped, x_d_1_up], dim=1)\n",
        "        x_d_1_conv_1=relu(self.d_1_conv_1(x_d_1_up_concat))\n",
        "        x_d_1_conv_2=relu(self.d_1_conv_2(x_d_1_conv_1))\n",
        "\n",
        "\n",
        "        x_d_2_up= self.d_2_up(x_d_1_conv_2)\n",
        "        crop_shape=x_d_2_up.shape\n",
        "        x_e_3_conv_2_cropped = x_e_3_conv_2[:, :, :crop_shape[2], :crop_shape[3]]\n",
        "        x_d_2_up_concat = torch.cat([x_e_3_conv_2_cropped, x_d_2_up], dim=1)\n",
        "        x_d_2_conv_1=relu(self.d_2_conv_1(x_d_2_up_concat))\n",
        "        x_d_2_conv_2=relu(self.d_2_conv_2(x_d_2_conv_1))\n",
        "\n",
        "        x_d_3_up= self.d_3_up(x_d_2_conv_2)\n",
        "        crop_shape=x_d_3_up.shape\n",
        "        x_e_2_conv_2_cropped = x_e_2_conv_2[:, :, :crop_shape[2], :crop_shape[3]]\n",
        "        x_d_3_up_concat = torch.cat([x_e_2_conv_2_cropped, x_d_3_up], dim=1)\n",
        "        x_d_3_conv_1=relu(self.d_3_conv_1(x_d_3_up_concat))\n",
        "        x_d_3_conv_2=relu(self.d_3_conv_2(x_d_3_conv_1))\n",
        "\n",
        "        x_d_4_up= self.d_4_up(x_d_3_conv_2)\n",
        "        crop_shape=x_d_4_up.shape\n",
        "        x_e_1_conv_2_cropped = x_e_1_conv_2[:, :, :crop_shape[2], :crop_shape[3]]\n",
        "        x_d_4_up_concat = torch.cat([x_e_1_conv_2_cropped, x_d_4_up], dim=1)\n",
        "        x_d_4_conv_1=relu(self.d_4_conv_1(x_d_4_up_concat))\n",
        "        x_d_4_conv_2=relu(self.d_4_conv_2(x_d_4_conv_1))\n",
        "\n",
        "        # 1x1 CONV\n",
        "        x_d_4_conv_3=self.d_4_conv_3(x_d_4_conv_2)\n",
        "\n",
        "        # print(\"decoder :D\")\n",
        "        # print(x_d_4_conv_3.shape)\n",
        "        # sys.exit()\n",
        "\n",
        "        # Apply softmax pixel-wise along the specified dimensions (usually channel dimension)\n",
        "        output = softmax(x_d_4_conv_3, dim=1)  # Apply softmax along the channel dimension\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "lCOQ5cGt7oJM",
        "outputId": "150a601e-b185-47e2-e894-35659cce6921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/59\n",
            "----------\n",
            "LR 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Target size (torch.Size([25, 6, 192, 192])) must be the same as input size (torch.Size([25, 6, 20, 20]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6da6157707ae>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/utilities.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(UNet)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0mexp_lr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set model to the evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utilities.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utilities.py\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(pred, target, metrics, bce_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mbce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([25, 6, 192, 192])) must be the same as input size (torch.Size([25, 6, 20, 20]))"
          ]
        }
      ],
      "source": [
        "import utilities\n",
        "\n",
        "utilities.run(UNet)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "TmJ1m_OXEwBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW46LAFO7oJM"
      },
      "source": [
        "### Grading Rubric (Total: 10 Marks)\n",
        "\n",
        "The lab is graded based on the following criteria:\n",
        "\n",
        "1. **Data Loading and Preparation (1 Mark)**\n",
        "   - Ensure reproducible results by setting seed to `27` before all randomized operations. (1 Mark)\n",
        "   \n",
        "2. **U-Net Architecture (4 Marks)**\n",
        "   - Uses appropriate U-Net Architecture to the problem with a full pipeline. (4 Marks)\n",
        "\n",
        "3. **Hyperparameters Tuning (2 Marks)**\n",
        "   - Report evaluation metrics on validation set. (1 Mark)\n",
        "   - Analyzes results and tunes hyperparameters. (1 Mark)\n",
        "  \n",
        "4. **Model Evaluation and Understanding (3 Marks)**\n",
        "   - Reports Jaccard Index and Dice Scores for test set. (1 Mark)\n",
        "   - **Comparison amongst your peers.** Compares the model's performance against those of peers to identify strengths and areas for improvement. (2 Marks)\n",
        "\n",
        "Each section of the lab will be evaluated on completeness, and correctness in approach and analysis. Part of the rubric also includes the student's ability to explain and justify their choices and results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHtnrgth7oJM"
      },
      "source": [
        "## Submission Instructions\n",
        "\n",
        "To ensure a smooth evaluation process, please follow these steps for submitting your work:\n",
        "\n",
        "1. **Prepare Your Submission:** Alongside your main notebook, include any additional files that are necessary for running the notebook successfully. This might include data files, images, or supplementary scripts.\n",
        "\n",
        "2. **Rename Your Files:** Before submission, please rename your notebook to reflect the IDs of the two students working on this project. The format should be `ID1_ID2`, where `ID1` and `ID2` are the student IDs. For example, if the student IDs are `9123456` and `9876543`, then your notebook should be named `9123456_9876543.ipynb`.\n",
        "\n",
        "3. **Check for Completeness:** Ensure that all required tasks are completed and that the notebook runs from start to finish without errors. This step is crucial for a smooth evaluation.\n",
        "\n",
        "4. **Submit Your Work:** Once everything is in order, submit your notebook and any additional files via the designated submission link on Google Classroom **(code: 2yj6e24)**. Make sure you meet the submission deadline to avoid any late penalties.\n",
        "5. Please, note that the same student should submit the assignments for the pair throughout the semester.\n",
        "\n",
        "By following these instructions carefully, you help us in evaluating your work efficiently and fairly **and any failure to adhere to these guidelines can affect your grades**. If you encounter any difficulties or have questions about the submission process, please reach out as soon as possible.\n",
        "\n",
        "We look forward to seeing your completed projects and wish you the best of luck!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}